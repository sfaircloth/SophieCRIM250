---
title: "Assignments"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
---
I want you to submit your assignment as a PDF, so I can keep a record of what the code looked like that day. I also want you to include your answers on your personal GitHub website. This will be good practice for editing your website and it will help you produce something you can keep after the class is over.

1. Download the Assignment1.Rmd file from Canvas. You can use this as a template for writing your answers. It's the same as what you can see on my website in the Assignments tab. Once we're done with this I'll edit the text on the website to include the solutions.

2. On RStudio, open a new R script in RStudio (File > New File > R Script). This is where you can test out your R code. You'll write your R commands and draw plots here.

3. Once you have finalized your code, copy and paste your results into this template (Assignment 1.Rmd). For example, if you produced a plot as the solution to one of the problems, you can copy and paste the R code in R markdown by using the ` ``{r} ``` ` command. Answer the questions in full sentences and Save.

4. Produce a PDF file with your answers. To do this, knit to PDF (use Knit button at the top of RStudio), locate the PDF file in your docs folder (it's in the same folder as the Rproj), and submit that on on Canvas in Assignment 1.

5. Build Website, go to GitHub desktop, commit and push. Now your solutions should be on your website as well.

# Assignment 1

_This assignment is due on Canvas on Monday 9/20 before class, at 10:15 am. Include the name of anyone with whom you collaborated at the top of the assignment._


### Problem 1 

_Install the datasets package on the console below using `install.packages("datasets")`. Now load the library._

```{r}
library(datasets)
```


_Load the USArrests dataset and rename it `dat`. Note that this dataset comes with R, in the package datasets, so there's no need to load data from your computer. Why is it useful to rename the dataset?_

**Answer**: Well, we want to replicate analyses. That’s why it’s nice to rename data.

```{r}
dat <- USArrests
```

### Problem 2

_Use this command to make the state names into a new variable called State. _
```{r}
library(dplyr)
dat$state <- tolower(rownames(USArrests))
```
_I used the dplyr package in order to rename this variable. I am sure there are other ways to do this but I used old reliable!_

_This dataset has the state names as row names, so we just want to make them into a new variable. We also make them all lower case, because that will help us draw a map later - the map function requires the states to be lower case._


_List the variables contained in the dataset `USArrests`._

```{r}
names(dat)
```
**Answer**: The four variables are Murder, Assault, UrbanPop, and Rape.

### Problem 3 

_What type of variable (from the DVB chapter) is `Murder`? 
_
**Answer**: quantitative variable

_What R Type of variable is it?_

```{r}
typeof(dat$Murder)
```
**Answer**: double


### Problem 4

_What information is contained in this dataset, in general? What do the numbers mean? _

**Answer**: The information in this dataset is a set of the number of arrests for different violent crimes in the United States, as divided by state. The numbers represent the numeric value of arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states, also providing the percent of the population living in urban areas (UrbanPop).

### Problem 5

_Draw a histogram of `Murder` with proper labels and title._

```{r}
hist(dat$Murder, col = 'red', ylab = 'Frequency', xlab = 'Murder Arrests per 100,000 residents', main = 'Histogram of Murder Arrests in the US', sub = '(by state)')
```

### Problem 6

_Please summarize `Murder` quantitatively. What are its mean and median? What is the difference between mean and median? What is a quartile, and why do you think R gives you the 1st Qu. and 3rd Qu.?_

```{r}
summary(dat$Murder)
```
**Answer**: The mean of the Murder variable is 7.788 (murder arrests per 100,000 residents of a state). The median is 7.250 (murder arrests per 100,000 residents of a state). The difference between these two values quantitatively is 0.538. The difference qualitatively is that the mean represents the average number of murder arrests per 100,000 residents of all US states while the median is the middle value of the data set, showing the data has a slight right skew. A quartile isdivides the dataset into four parts, with Q1 being the median of the lower half of the dataset and Q3 being the median of the upper half of the data set. R gives you this information because quartiles can be helpful in determining where a specific data point falls in the set; for example, if a value for "Murders" is greater than Q3, 11.250, we can more reasonably assume that this state has an exceptionally high murder rate in comparison to other states in the US. If a state has a value for "Murders" that is less than 4.075, we can more reasonably assume that this state is relatively safer than other US states!

### Problem 7

_Repeat the same steps you followed for `Murder`, for the variables `Assault` and `Rape`. Now plot all three histograms together. You can do this by using the command `par(mfrow=c(3,1))` and then plotting each of the three. _

```{r, echo = TRUE, fig.width = 5, fig.height = 8}
hist(dat$Assault, col = 'red', ylab = 'Frequency', xlab = 'Assault Arrests per 100,000 residents', main = 'Histogram of Assault Arrests in the US', sub = '(by state)')
summary(dat$Assault)
hist(dat$Rape, col = 'red', ylab = 'Frequency', xlab = 'Rape Arrests per 100,000 residents', main = 'Histogram of Rape Arrests in the US', sub = '(by state)')
summary(dat$Rape)
par(mfrow=c(3,1))
hist(dat$Murder, col = 'red', ylab = 'Frequency', xlab = 'Murder Arrests per 100,000 residents', main = 'Histogram of Murder Arrests in the US', sub = '(by state)')
hist(dat$Assault, col = 'red', ylab = 'Frequency', xlab = 'Assault Arrests per 100,000 residents', main = 'Histogram of Assault Arrests in the US', sub = '(by state)')
summary(dat$Assault)
hist(dat$Rape, col = 'red', ylab = 'Frequency', xlab = 'Rape Arrests per 100,000 residents', main = 'Histogram of Rape Arrests in the US', sub = '(by state)')
```

_What does the command par do, in your own words (you can look this up by asking R `?par`)?_

**Answer**: The par function allows an R user to look at the graphical parameters that control how graphs are displayed. In the case of the par(mfrow=c(3,1)) function, we are able to not only look at the graphical parameters, but control them so that we can decide how many subplots we want displayed.

_What can you learn from plotting the histograms together?_

**Answer**: From plotting the histograms together, we can view a comparison of the distributions in relation to each other, examining skew and quartile arrangement to see how distributions vary by crime type. On a more general note, plotting the histograms together allows us to see generally how frequency and distribution relationships among multiple variables interact, allowing a viewer a more clear image of the relationship between variables while still being able to view a more individualistic look at each histogram.
  
### Problem 8

_In the console below (not in text), type `install.packages("maps")` and press Enter, and then type `install.packages("ggplot2")` and press Enter. This will install the packages so you can load the libraries._

_Run this code:_

```{r, fig.width = 7.5, fig.height = 4}

library(maps) 
library(ggplot2) 

ggplot(dat, aes(map_id=state, fill=Murder)) + 
  geom_map(map=map_data("state")) + 
  expand_limits(x=map_data("state")$long, y=map_data("state")$lat)
```

_What does this code do? Explain what each line is doing._

**Answer**: ggplot is the most basic implementation of any data visualization in R. In this instance, the first line of code is filling out the basic information of the data visual by specifying the variables which are involved in the plot. The second line of code employs the maps package and ggplot2 package to choose specifically to create a mapping of the selected variables, using states as the selected implementation of the map. The third line employs the expand_limit function in order to ensure that the limits of the visualization include a single value for all aspects of the plot, implying what should be included in the scale.

$$\\[2in]$$

# Assignment 2

Instructions: Copy your code, paste it into a Word document, and turn it into Canvas. You can turn in a .docx or .pdf file. Show any EDA (graphical or non-graphical) you have used to come to this conclusion.


## Problem 1: Load data

Set your working directory to the folder where you downloaded the data.
```{r}
setwd("~/Documents/GitHub/SophieCRIM250")
```

Read the data
```{r}
dat <- read.csv(file = 'dat.nsduh.small.1.csv')
```
What are the dimensions of the dataset? 

```{r}
names(dat)
dim(dat)
```

- The dimensions are 171 x 7

## Problem 2: Variables

Describe the variables in the dataset.
- mjage is a variable that denotes how old someone was the first time they used marijuana or hashish
- cigage is a variable that denotes how old someone was when they first started smoking cigarettes every day
- iralcage is a variable that denotes how old someone was when they first tried alcohol, numbers greater than 900 implying never used or lack of answer
- age2 is a variable that splits people up into groups based on respondent age
- sexatract is a variable that splits people into groups based on sexual attraction, each number denoting a different sexual preference
- speakengl is a variable that splits people into groups based off of their english literacy
- irsex is a variable that splits people into two different groups based on a binary gender system, 1 being male and 2 being female

What is this dataset about? Who collected the data, what kind of sample is it, and what was the purpose of generating the data?
- This dataset is about the health and drug use statistics of the United States. 
- This data is from the National Survey on Drug Use and Health, conducted by The Substance Abuse and Mental Health Services Administration in the US Department of Health and Human Services.
- This is a random sample.
- The data was generated for the sake of government agencies, private organizations, individual researchers, and the public at large for a number of different purposes.
- This data is used mainly to provide more information on substance use and demographic statistics in the United States.

## Problem 3: Age and gender

What is the age distribution of the sample like? Make sure you read the codebook to know what the variable values mean.
```{r}
hist(dat$age2)
summary(dat$age2)
```
- The age distribution is left skewed, with a mean of 13.98 (representing ages 26-29) and median of 15 (representing ages 35-49). 
# Do you think this age distribution representative of the US population? Why or why  not?
- I do feel like this sample is representative. The distribution of ages in the age set are misleading; the ages of 18-49 (as represented by groups 7-15) are the majority of the sample, which is consistent with the most recent US Census. 
# Is the sample balanced in terms of gender? If not, are there more females or males?
```{r}
summary(dat$irsex)
```
- This sample is fairly balanced in terms of gender. The mean for the irsex variable is 1.468, meaning the sample is fairly balanced in terms of sex.
# Use this code to draw a stacked bar plot to view the relationship between sex and age. What can you conclude from this plot?
```{r}
tab.agesex <- table(dat$irsex, dat$age2)
barplot(tab.agesex,
        main = "Stacked barchart",
        xlab = "Age category", ylab = "Frequency",
        legend.text = rownames(tab.agesex),
        beside = FALSE) # Stacked bars (default)
```
- This plot demonstrates that the distribution is fairly balanced in age, with the majority of participants on either tail of the data being mainly males.

## Problem 4: Substance use

For which of the three substances included in the dataset (marijuana, alcohol, and cigarettes) do individuals tend to use the substance earlier?
```{r}
par(mfrow=c(3,1))
hist(dat$mjage, col = 'red', ylab = 'Frequency', xlab = 'Age category', main = 'Histogram of Marijuana Usage')
hist(dat$cigage , col = 'red', ylab = 'Frequency', xlab = 'Age category', main = 'Histogram of Cigarette Usage')
hist(dat$iralcage, col = 'red', ylab = 'Frequency', xlab = 'Age category', main = 'Histogram of Alcohol Consumption')
```
- Individuals tend to use marijuana at the youngest age out of the three substances.

## Problem 5: Sexual attraction
```{r}
library(ggplot2)
```
## What does the distribution of sexual attraction look like? Is this what you expected?
```{r}
hist(dat$sexatract)
summary(dat$sexatract)
table(dat$sexatract)
```
- According to the numeric system for sexual attraction, this gives the distribution a strong right skew.
- This is not a necessarily surprising result, considering there is generally a strong heterosexual representation in the United States, especially on representative surveys.
- While I do not feel this is truly representative of the United States, there are many people who may feel uncomfortable correctly identifying sexual preference on a survey used by government organizations and regarding personal information (even if it is later encoded).

## What is the distribution of sexual attraction by gender? 
```{r}
genatt <- table(dat$irsex, dat$sexatract)
barplot(genatt,
        main = "Stacked barchart",
        xlab = "Gender", ylab = "Frequency",
        legend.text = rownames(genatt),
        beside = FALSE) # Stacked bars (default)
sum(dat$sexatract == 1 & dat$irsex == 1)
- 82
sum(dat$sexatract == 1 & dat$irsex == 2)
- 54
```
- The distribution of sexual attraction is highly populated by those who identified (via the codebook) as only being attracted to the opposite gender.
- A smaller number of women identify as being heterosexual than men, thought this could be slightly biased due to a smaller sample size of women.
## Problem 6: English speaking

What does the distribution of English speaking look like in the sample? Is this what you might expect for a random sample of the US population?
summary(dat$speakengl)
table(dat$speakengl)
- Majority of the sample are English speakers, with 169 of the 171 respondents self-identifying as speaking English well or very well and only two respondents not speaking English well.
- This is not what I would expect from a random sample of the US, but makes sense in the fact that this is a survey administered in English, probably leading to some sampling bias in a high yield of English speakers.
Are there more English speaker females or males?
```{r}
sum(dat$speakengl == 1 & dat$irsex == 1)
- 84
sum(dat$speakengl == 1 & dat$irsex == 2)
- 77
sum(dat$speakengl == 2 & dat$irsex == 1)
- 7
sum(dat$speakengl == 2 & dat$irsex == 2)
- 1
sum(dat$speakengl == 3 & dat$irsex == 1)
- 0
sum(dat$speakengl == 3 & dat$irsex == 2)
- 2
```
- Assuming that we say that "not an English speaker" is characterized by someone self-assessing themselves as speaking English "not well," there is a higher number of female non-English speakers.
- This means that, from this very limited data set, there is a higher number of male English speakers.





# Exam 1

## Instructions

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (fatal-police-shootings-data.csv) onto that folder, and save your Exam 1.Rmd file in the same folder.

c. Download the README.md file. This is the codebook. 

d. Load the data into an R data frame.
```{r}
dat <- read.csv("~/Documents/GitHub/SophieCRIM250/fatal-police-shootings-data.csv")
```


## Problem 1 (10 points)

a. Describe the dataset. This is the source: https://github.com/washingtonpost/data-police-shootings . Write two sentences (max.) about this.

__This data is a recording of fatal shootings by police officers in the line of duty since Jan. 1, 2015. This data, as recorded in the Washington Post database, is separated by circumstances under which the fatality ensued.__

b. How many observations are there in the data frame?
```{r}
dim(dat)
```

__There are 6594 observations in this data set, categorized by 17 different variables.__

c. Look at the names of the variables in the data frame. Describe what "body_camera", "flee", and "armed" represent, according to the codebook. Again, only write one sentence (max) per variable.
```{r}
names(dat)
```

__body_camera: This variable indicates whether or not the officer involved in the incident was wearing a body camera.__
__flee: This variable indicates whether or not the victim appeared to be moving away from the officer at the time of the shooting, divided into three possibilities: fleeing in a car, fleeing on foot, not fleeing.__
__armed: This variable indicates whether or not a victim was considered to be armed at the time of the shooting and if so, what they were armed with.__

d. What are three weapons that you are surprised to find in the "armed" variable? Make a table of the values in "armed" to see the options.
```{r}
armeddat <- table(dat$armed)
armeddat
```

__In the armed variable, I was most surprised to see that people were "armed" with a pitchfork (very medieval), a pen (???), and an air conditioner.__

## Problem 2 (10 points)

a. Describe the age distribution of the sample. Is this what you would expect to see?
```{r}
hist(dat$age)
```

__The distribution shows a right skew to the ages of victims in recorded fatal shootings. I feel like this is fairly predictable, as the average age in the US is 38, and we know that fatal shootings often occur in over-policed low-income areas where the majority of people who are out and about are the working class of these areas, putting majority of their ages around 20-40 years old.__

b. To understand the center of the age distribution, would you use a mean or a median, and why? Find the one you picked.
```{r}
summary(dat$age)
```

__I would use the median of the data. Because the data is skewed, the median is a better measure of central tendency than the mean as it is more representative of the sample given what we know about it's distribution. The median is 35, as in 35 years old is the average age of someone involved in a fatal shooting in the United States at the hand of police.__

c. Describe the gender distribution of the sample. Do you find this surprising?
```{r}
table(dat$gender)
```

__I do find this surprising. Given that there is a fairly predictable distribution of ages in this data set, you would assume there be an equally as predictable distribution of gender. However, knowing what we know about fatal shootings by police in the United States, the narrative portrayed by police officers often follows the line of not-being-confident-in-a-lack-of-threat-from-the-victim. While this narrative is incredibly frustrating, it does align with the uneven distribution of men and women in this event because women often are portrayed as being a lower threat level than men in general.__


## Problem 3 (10 points)

a. How many police officers had a body camera, according to news reports? What proportion is this of all the incidents in the data? Are you surprised that it is so high or low?

```{r}
table(dat$body_camera)
910/6594
# prop.table(dat$body_camera)
# would have to convert to numeric, can just get proportion via simple math
```

__According to the news, 910 of the officers had a body camera at the time of the incident. This is 13.800% of the incidents in this data. This proportion being so low is surprising to me because I thought that it was protocol as of the past few years for officers to wear body cameras when on duty. The fact that this statistic is so low is very concerning.__

b. In  how many of the incidents was the victim fleeing? What proportion is this of the total number of incidents in the data? Is this what you would expect?
```{r}
table(dat$flee)
1058 + 845
1903/6594
```

__The victim was fleeing in 1058 of the incidents (either by car or on foot: "other" answers and not included answers on this were excluded due to lack of information). This is 28.860% of the data set. This is not surprising given what we know about police brutality in the United States, but it is very concerning. This means that over 70% of victims who were shot fatally by police were not fleeing.__



## Problem 4 (10 points) -  Answer only one of these (a or b).

a. Describe the relationship between the variables "body camera" and "flee" using a stacked barplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the options for "flee", each bar contains information about whether the police officer had a body camera (vertically), and the height along the y-axis shows the frequency of that category).*

*Hint 2: Also, if you are unsure about the syntax for barplot, run ?barplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}
counts <- table(dat$body_camera, dat$flee)
barplot(counts, col=c("red", "blue"), legend=TRUE, xlab="Flee", ylab="Frequency")
```

__From this relationship, we can see that there is an even proportion of body cam presence across all flee categories, demonstrating that the presence of a body cam does not seem to affect whether or not someone in this study fled or not. Though there is a section in the distribution that is unlabelled due to the flee variable being unmarked in these instances, even this variable demonstrates an even distribution.__

b. Describe the relationship between age and race by using a boxplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the race categories and the height along the y-axis is age.* 

*Hint 2: Also, if you are unsure about the syntax for boxplot, run ?boxplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}

```

__Your answer here.__






## Extra credit (10 points)

a. What does this code tell us? 

```{r, eval=FALSE}
mydates <- as.Date(dat$date)
head(mydates)
(mydates[length(mydates)] - mydates[1])
```

__This code tells us the difference in time between the first and last recorded date in the data set.__

b. On Friday, a new report was published that was described as follows by The Guardian: "More than half of US police killings are mislabelled or not reported, study finds." Without reading this article now (due to limited time), why do you think police killings might be mislabelled or underreported?

__Unfortunately, the reach of the police department is seen in the representation of police killings in reportings. Because an accurate report of fatal shootings at the hands of police relies on a pure and fully not corrupted police force and governmental power, there would be several levels of responsibility that would have to be upheld to ensure that all incidents of fatal shootings by police would be reported accurately. As the article by The Guardian says, "The same government responsible for this violence is also responsible for reporting on it."__

c. Regarding missing values in problem 4, do you see any? If so, do you think that's all that's missing from the data?

__There is visibly missing data in the flee variable. This is likely not all the data that is missing from the data set because it is not rare for data to be missing in such a large data set like this.__

# Assignment 3

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```


This assignment is due on Canvas on Wednesday 10/27/2021 before class, at 10:15 am. Include the name of anyone with whom you collaborated at the top of the assignment.

Submit your responses as either an HTML file or a PDF file on Canvas. Also, please upload it to your website.

Save the file (found on Canvas) crime_simple.txt to the same folder as this file (your Rmd file for Assignment 3).

Load the data.
```{r}
library(readr)
library(knitr)
dat.crime <- read_delim("~/Documents/GitHub/SophieCRIM250/crime_simple.txt", delim = "\t")
```

This is a dataset from a textbook by Brian S. Everitt about crime in the US in 1960. The data originate from the Uniform Crime Report of the FBI and other government sources. The data for 47 states of the USA are given. 

Here is the codebook:

R: Crime rate: # of offenses reported to police per million population

Age: The number of males of age 14-24 per 1000 population

S: Indicator variable for Southern states (0 = No, 1 = Yes)

Ed: Mean of years of schooling x 10 for persons of age 25 or older

Ex0: 1960 per capita expenditure on police by state and local government

Ex1: 1959 per capita expenditure on police by state and local government

LF: Labor force participation rate per 1000 civilian urban males age 14-24

M: The number of males per 1000 females

N: State population size in hundred thousands

NW: The number of non-whites per 1000 population

U1: Unemployment rate of urban males per 1000 of age 14-24

U2: Unemployment rate of urban males per 1000 of age 35-39

W: Median value of transferable goods and assets or family income in tens of $

X: The number of families per 1000 earning below 1/2 the median income


We are interested in checking whether the reported crime rate (# of offenses reported to police per million population) and the average education (mean number of years of schooling for persons of age 25 or older) are related. 


1. How many observations are there in the dataset? To what does each observation correspond?

```{r}
library(dplyr)
count(dat.crime)
```

__There are 47 observations in the dataset. The observations correspond to the different rows, aka different states.__

2. Draw a scatterplot of the two variables. Calculate the correlation between the two variables. Can you come up with an explanation for this relationship?

```{r, fig.width=6, fig.height=3.5}
library(datasets)
plot(dat.crime$Ed,dat.crime$R,  main="Relationship between Reported Crime Rate and Average Education for 47 States",
    xlab="Average Education (mean number of years of schooling for persons of age 25 or older)", ylab="Crime Rate (# of offenses reported to police per million population)")
x <- cor(dat.crime$R, dat.crime$Ed)
x
```

__I cannot come up with an explanation for this relationship on first impression. The correlation of these two variables is 0.3228349, meaning that there is a fairly weak correlation. Because there are so many other factors in this dataset, we can assume that there is likely another factor that correlates more highly to the reported crime rate, but this low of a correlation does not allow us to make any causal inferences.__

3. Regress reported crime rate (y) on average education (x) and call this linear model `crime.lm` and write the summary of the regression by using this code, which makes it look a little nicer `{r, eval=FALSE} kable(summary(crime.lm)$coef, digits = 2)`.

```{r} 
# Remember to remove eval=FALSE above!

dat.crime$R.c = scale(dat.crime$R, center=TRUE, scale=FALSE)
crime.lm <- lm(formula = Ed ~ R.c, data = dat.crime) 
kable(summary(crime.lm)$coef, digits = 2)
summary(crime.lm)
```

4. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.)

```{r} 
# plot 1 & plot 2, residuals vs x
plot(dat.crime$R.c, crime.lm$residuals, ylim=c(-15,15), main="Residuals vs. x", xlab="x, Scaled Crime Rates", ylab="Residuals")
abline(h = 0, lty="dashed")
# plot 3
plot(crime.lm, which=3)
# plot 4, qq and outlier condition
plot(crime.lm, which=2)
plot(crime.lm, which=5)
```

__The four assumptions are as follows: linearity, independence, equal variance, and normal population. The first assumption - linearity - is not satisfied because the first plot of the residuals vs x does not have a horizontal direction. The second assumption - independence - is satisfied because the plot of residuals vs x does not display any distinct patterns. The third assumption - equal variance - is satisfied because the flat line shows that the errors are of constant variance. The fourth assumption - normal population - is not satisfied because the qq plot is heavily tailed, meaning it does not follow the normal model well.__

5. Is the relationship between reported crime and average education statistically significant? Report the estimated coefficient of the slope, the standard error, and the p-value. What does it mean for the relationship to be statistically significant?

__The estimated coefficient is 0.09. The standard error is 0.04. The p-value is 0.02688. This means that this relationship is statistically significant, as the p-value is less than 0.05. If the relationship is statistically significant, this means that there is a high likelihood that the observance is not wrong or random (I use this word carefully) chance.__

6. How are reported crime and average education related? In other words, for every unit increase in average education, how does reported crime rate change (per million) per state?

__For every unit increase in average education, the reported crime rate increases by 0.09 units, meaning that the reported crime rate increases by 0.09 offenses reported to police per million population.__

7. Can you conclude that if individuals were to receive more education, then crime will be reported more often? Why or why not?

__We cannot conclude this from the dataset. While it appears there is a statistically significant correlation, this does not mean we can assume causation. While the variables correlate to each other, this is not conducive of a causal relationship.__

# Exam 2

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (sim.data.csv) onto that folder, and save your Exam 2.Rmd file in the same folder.

c. Data description: This dataset provides (simulated) data about 200 police departments in one year. It contains information about the funding received by the department as well as incidents of police brutality. Suppose this dataset (sim.data.csv) was collected by researchers to answer this question: **"Does having more funding in a police department lead to fewer incidents of police brutality?"**
d. Codebook:
- funds: How much funding the police department received in that year in millions of dollars.
- po.brut: How many incidents of police brutality were reported by the department that year.
- po.dept.code: Police department code

## Problem 1: EDA (10 points) 

Describe the dataset and variables. Perform exploratory data analysis for the two variables of interest: funds and po.brut.

```{r}
dat <- read.csv(file = 'sim.data.csv')
dim(dat)
names(dat)
summary(dat$funds)
summary(dat$po.brut)
```

__The dataset consists of 200 observations (rows) of three different variables (columns). This dataset contains simulated data on 200 police departments in year, providing information on their funding received and police brutality rates. The variable funds refers to the amount of funding received by the police department in a year in millions of dollars. The variable po.brut refers to how many instances of police brutality were reported in the department for the same year..__


## Problem 2: Linear regression (30 points)

a. Perform a simple linear regression to answer the question of interest. To do this, name your linear model "reg.output" and write the summary of the regression by using "summary(reg.output)". 

```{r}
# Remember to remove eval=FALSE!!
library(datasets)
library(dplyr)
reg.output <- lm(formula = po.brut ~ funds, data = dat)
summary(reg.output)
```

__Does having more funding in a police department lead to fewer incidents of police brutality? The linear regression of these two variables leads us to believe that there is a strong possibility that there is a correlation between funding in a police department and instances of police brutality. It is unclear yet whether this is a causal relationship or not.__

b. Report the estimated coefficient, standard error, and p-value of the slope. Is the relationship between funds and incidents statistically significant? Explain.

__The estimated coefficient is -2.6455. The standard error is 0.0324. The p-value is <2e-16. This relationship is very obviously statistically significant, as the p-value is significantly less than 0.05.__

c. Draw a scatterplot of po.brut (x-axis) and funds (y-axis). Right below your plot command, use abline to draw the fitted regression line, like this:
```{r, fig.width=4, fig.height=4}
# Remember to remove eval=FALSE!!

library(datasets)
plot(dat$funds, dat$po.brut,  main="Relationship between Police Department Funding and Reports of Police Brutality",
    xlab="Funding (millions of dollars per year)", ylab="Reported Instances of Police Brutality per year")
abline(reg.output, col = "red", lwd=2)
```
Does the line look like a good fit? Why or why not?

__This line does look like a good fit. The plot generally follows the plots on the line, accounting for the curvature of the scatter plot in its slope.__

d. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.) If not, what might you try to do to improve this (if you had more time)?

```{r, fig.width=4, fig.height=4}
# plot 1 & plot 2, residuals vs x
plot(dat$funds, dat$po.brut, ylim=c(-15,15), main="Residuals vs. x", xlab="x, Funding (millions of dollars per year)", ylab="Residuals")
abline(h = 0, lty="dashed")
# plot 3
plot(reg.output, which=3)
# plot 4, qq and outlier condition
plot(reg.output, which=2)
plot(reg.output, which=5)
```
__The four assumptions are as follows: linearity, independence, equal variance, and normal population. The first assumption - linearity - is satisfied because the first plot of the residuals vs x has a straight line of plots. The second assumption - independence - is satisfied because the plot of residuals vs x does not display any distinct patterns. The third assumption - equal variance - is not satisfied because the lack of a flat line shows that the errors are not of constant variance. The fourth assumption - normal population - is debatably not satisfied because the residuals vs leverage plot shows that there are no influential cases because all cases are well inside of Cook's distance lines.__

e. Answer the question of interest based on your analysis.

__Given the information we have, despite it being questionable whether or not all assumptions of linear regression are satisfied, we can assume that there is a strong correlation between reports of police brutality and funding received by a police department. Because there is such a statistically significant p-value of the dataset, we are able to determine that there is a relationship between these two variables. However, we can not determine that there is a causal relationship between the two because the four assumptions of linear regression are not met - this means we cannot rely as heavily on our linear model's causality prediction.__

## Problem 3: Data ethics (10 points)

Describe the dataset. Considering our lecture on data ethics, what concerns do you have about the dataset? Once you perform your analysis to answer the question of interest using this dataset, what concerns might you have about the results?

__The dataset consists of 200 observations (rows) of three different variables (columns). This dataset contains simulated data on 200 police departments in year, providing information on their funding received and police brutality rates. From our data ethics checklist, I worry that this dataset does not satisfy our fairness question of "Have we studied and understood possible sources of bias in our data?" This dataset is dealing with a sensitive topic - police brutality. We discussed during our ethics lecture that even AIs which are designed to do crime-related analysis have inherent biases based off of previous biased information. This dataset does not take into account the fact that there is the possibility that the location of the police department could also play a large role in the instances of police brutality, maybe more so than funding. We know that police brutality happens disproportionately in underrepresented minority neighborhoods. This dataset does not acknowledge this highly likely bias, a factor which could conflate the relationship between the two variables in question.__



# Assignment 4
## 3 (pt 1)

#### loading the library of tidyverse where the tidyverse package is stored to access ggplot

library(tidyverse)

#### installing the package so one can reload it after every session

install.packages("tidyverse")

#### calling mpg dataframe A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). mpg contains observations collected by the US Environmental Protection Agency on 38 models of cars.
#### mapping a gg scatterplot that maps car's engine size against fuel efficiency
#### shows a negative relationship between x and y variables 

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))

#### creating a reusable template for making graphs with ggplot2

ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))

#### mapping aesthetics in your plot to the variables in the dataset, maps class to color aesthetic 

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))

#### mapping class against size aesthetic 

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, size = class))

#### mapping class to the alpha aesthetic, which controls the transparency of the points, or to the shape aesthetic, which controls the shape of the points 

#### left

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))

#### right

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, shape = class))

#### setting aesthetic properties of geom manually, making all of the points blue

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue")

#### missed parentheses so the point do not come out blue

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))

#### plus sign must come at the end of the line not the beginning

ggplot(data = mpg) 
+ geom_point(mapping = aes(x = displ, y = hwy))

#### facetting plot by a single variable. First argument is a formula, followed by ~ followed by variable name where formula is the name of a data structure in R/ The variable that is passed to facet_wrap should be discrete 

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)

#### Facetting your plot on the combination of two variables, add facet_grid() to your plot call. The first argument of facet_grid() is also a formula. This time the formula should contain two variable names separated by a ~. The empty grids show that there is no data attributed to this specific variable 

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)

#### Code creates 4 facet columns that are generated by the "." The data maps engine size against fuel efficiency

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(drv ~ .)
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(. ~ cyl)

#### facets columns and rows of each type of car variable, engine size against fuel efficiency 

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)

#### point geom representation of the data
#### left

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))

#### right smooth geom representation of the data

ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy))

#### drawing a different line, with a different linetype, for each unique value of the variable that you map to linetype.

ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))

#### geom smooth uses a single geometric object to display multiple rows of data.
#### Setting the group aesthetic to a categorical variable to draw multiple objects.
#### ggplot2 draws a separate object for each unique value of the grouping variable.

#### In practice, ggplot2 will automatically group the data for these geoms whenever you map an aesthetic to a discrete variable (linetype example)

ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy))

ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))

ggplot(data = mpg) +
  geom_smooth(
    mapping = aes(x = displ, y = hwy, color = drv),
    show.legend = FALSE
  )

#### displaying multiple geoms in the same plot by adding multiple geom functions to ggplot

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  geom_smooth(mapping = aes(x = displ, y = hwy))

#### Avoiding duplication of variables by passing a set of mappings to ggplot()

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point() + 
  geom_smooth()

#### Placing mappings in a geom function, ggplot2 treats them as local mappings for the layer. Uses these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers. 

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth()

#### smooth line displays just a subset of the mpg dataset, the subcompact cars.

#### local data argument in geom_smooth() overrides the global data argument in ggplot() for that layer only.

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
geom_point(mapping = aes(color = class)) + 
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)

#### Point plot with smooth line mapping separated by color

ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + 
  geom_point() + 
  geom_smooth(se = FALSE)

#### Same graph by avoiding duplication of variables by passing in the mapping function

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point() + 
  geom_smooth()

ggplot() + 
  geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))

#### generates bar plot of the diamonds dataset showing that  more diamonds are available with high quality cuts than with low quality cuts

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut))

#### Using geoms and stats interchangeably. You can recreate the previous plot using stat_count() instead of geom_bar()

ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))

#### Overriding the default stat. change the stat of geom_bar() from count (the default) to identity. Lets one map the height of the bars to the raw values of a y variable

demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)

ggplot(data = demo) +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")

#### displaying a bar chart of proportion rather than count

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))

#### summarises the y values for each unique x value, to draw attention to the summary that you’re computing

ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.min = min,
    fun.max = max,
    fun = median
  )

#### creates proportion bar chart that is color coordinated where proportion is = 1 because proportions cannot exceed 100% 

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = after_stat(prop)))
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = color, y = after_stat(prop)))

#### Coloring bars using function colour and fill 

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, colour = cut))
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = cut))

#### map the fill aesthetic to another variable, like clarity: the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity))

#### position = "identity" will place each object exactly where it falls in the context of the graph. Makes them transparent for easier viewing

ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + 
  geom_bar(alpha = 1/5, position = "identity")
ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + 
  geom_bar(fill = NA, position = "identity")

#### position = "fill" works like stacking, but makes each set of stacked bars the same height.

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")

#### position = "dodge" places overlapping objects directly beside one another.

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")

#### position = "jitter" adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noise.

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")

#### point plot that does not include jittering to add random noise

ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + 
  geom_point()

#### coord_flip() switches the x and y axes. This is useful (for example), if you want horizontal boxplots

ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot()
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()

#### oord_quickmap() sets the aspect ratio correctly for maps.

nz <- map_data("nz")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()

#### coord_polar() uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart

bar <- ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_flip()
bar + coord_polar()

#### generates a regression line for the plot - shows that there is not a good relationship between the two variables. Creates a point plot without randomness

ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline() +
  coord_fixed()

## 28 (pt 2)
library(tidyverse)

#### this line is loading the package 'tidyverse' which we already have installed

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(title = "Fuel efficiency generally decreases with engine size")

#### this section is making a scatterplot with the function geom_point and then making a line of best fit using the geom_smooth function
#### this section also employs the labs function to make the label for the title of the graph

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Fuel efficiency generally decreases with engine size",
    subtitle = "Two seaters (sports cars) are an exception because of their light weight",
    caption = "Data from fueleconomy.gov"
  )

#### this section does the same as the last - the only difference is that it uses the labs function to also add a subtitle and caption to the graph

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  labs(
    x = "Engine displacement (L)",
    y = "Highway fuel economy (mpg)",
    colour = "Car type"
  )

#### this section does the same as the last - the only difference is that it uses x and y to define the axes labels and makes the legend label "car type"

df <- tibble(
  x = runif(10),
  y = runif(10)
)
ggplot(df, aes(x, y)) +
  geom_point() +
  labs(
    x = quote(sum(x[i] ^ 2, i == 1, n)),
    y = quote(alpha + beta + frac(delta, theta))
  )

#### this section is creating a data frame using the tibble function
#### it also uses mathematical functions in place of strings for the axes using the quote function

best_in_class <- mpg %>%
  group_by(class) %>%
  filter(row_number(desc(hwy)) == 1)

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_text(aes(label = model), data = best_in_class)

#### this section pulls out the highest rated of each class, in this case the most efficient cars

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.5)

#### this section uses the geom_label function to draw a rectangle behind the text, with nudge_y moving the labels above the corresponding points

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_point(size = 3, shape = 1, data = best_in_class) +
  ggrepel::geom_label_repel(aes(label = model), data = best_in_class)
  
#### this section uses the ggrepel package to automatically adjust labels so they don't overlap

class_avg <- mpg %>%
  group_by(class) %>%
  summarise(
    displ = median(displ),
    hwy = median(hwy)
  )


ggplot(mpg, aes(displ, hwy, colour = class)) +
  ggrepel::geom_label_repel(aes(label = class),
                            data = class_avg,
                            size = 6,
                            label.size = 0,
                            segment.color = NA
  ) +
  geom_point() +
  theme(legend.position = "none")
  
#### this section replaces the legend with labels on the plot

label <- mpg %>%
  summarise(
    displ = max(displ),
    hwy = max(hwy),
    label = "Increasing engine size is \nrelated to decreasing fuel economy."
  )

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")
  
#### this section adds a single label to the plot by creating a new data frame

label <- tibble(
  displ = Inf,
  hwy = Inf,
  label = "Increasing engine size is \nrelated to decreasing fuel economy."
)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")
  
#### this section places the text we just created directly on the borders of the plot

"Increasing engine size is related to decreasing fuel economy." %>%
  stringr::str_wrap(width = 40) %>%
  writeLines()
  
#### this section adds line breaks given the number of characters

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  scale_x_continuous() +
  scale_y_continuous() +
  scale_colour_discrete()
  
#### this section chooses scales that are not the default

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_y_continuous(breaks = seq(15, 40, by = 5))
  
#### this section uses the break function to override the default choice of axes ticks

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_x_continuous(labels = NULL) +
  scale_y_continuous(labels = NULL)
  
#### this section uses labels = NULL to get rid of the numeric tick labels on the axes

presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id)) +
  geom_point() +
  geom_segment(aes(xend = end, yend = id)) +
  scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y")
  
#### this section uses the breaks function to highlight the observations in the data set

base <- ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class))

base + theme(legend.position = "left")
base + theme(legend.position = "top")
base + theme(legend.position = "bottom")
base + theme(legend.position = "right") # the default

#### this section controls where the legend is drawn

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  theme(legend.position = "bottom") +
  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4)))
  
#### uses nrow to control the number of rows and overrides the aesthetic using override.aes

ggplot(diamonds, aes(carat, price)) +
  geom_bin2d()

ggplot(diamonds, aes(log10(carat), log10(price))) +
  geom_bin2d()
  
#### this section log transforms the data to make it easier to see the precise relationship

ggplot(diamonds, aes(carat, price)) +
  geom_bin2d() + 
  scale_x_log10() + 
  scale_y_log10()
  
#### we use this to get rid of the log axes labels and rescale the axes

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv))

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv)) +
  scale_colour_brewer(palette = "Set1")
  
#### this section adjusts the colors of the graph using the palette function

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv, shape = drv)) +
  scale_colour_brewer(palette = "Set1")
  
#### this section adds a redundant shape mapping using the shape function

presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id, colour = party)) +
  geom_point() +
  geom_segment(aes(xend = end, yend = id)) +
  scale_colour_manual(values = c(Republican = "red", Democratic = "blue"))
  
#### this section sets the colors of the data set to match the standard mapping of party with colour = party

df <- tibble(
  x = rnorm(10000),
  y = rnorm(10000)
)
ggplot(df, aes(x, y)) +
  geom_hex() +
  coord_fixed()

ggplot(df, aes(x, y)) +
  geom_hex() +
  viridis::scale_fill_viridis() +
  coord_fixed()
  
#### this section emplys a colour scheme that uses continous colors

ggplot(mpg, mapping = aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth() +
  coord_cartesian(xlim = c(5, 7), ylim = c(10, 30))

mpg %>%
  filter(displ >= 5, displ <= 7, hwy >= 10, hwy <= 30) %>%
  ggplot(aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth()
  
#### this section uses coord_cartesian to zoom in on a specific region of the plot

suv <- mpg %>% filter(class == "suv")
compact <- mpg %>% filter(class == "compact")

ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point()

ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point()
  
#### this section subsets the data

x_scale <- scale_x_continuous(limits = range(mpg$displ))
y_scale <- scale_y_continuous(limits = range(mpg$hwy))
col_scale <- scale_colour_discrete(limits = unique(mpg$drv))

ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale

ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale
  
#### this section shares the scales of the plots across different plots using the limits function

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  theme_bw()
  
#### this section uses theme_bw to change the theme of the plot

ggplot(mpg, aes(displ, hwy)) + geom_point()

#### this section just creates a ggplot as we have been doing

ggsave("my-plot.pdf")

#### this section uses the ggsave function to save our current plot as an image

